{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键词搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "Keyword = \"The Wandering Earth\"              # 搜索关键词\n",
    "Start_date = \"2023-03-21\"       # 开始日期\n",
    "End_date = \"2023-04-20\"         # 结束日期\n",
    "Email = \"\"                      # 推特账号邮箱\n",
    "Username = \"\"                   # 推特账号@之后的名字\n",
    "Password = \"\"                   # 推特账号密码\n",
    "cookie = ''                     # 替换为你的 Cookie\n",
    "\n",
    "# 设置代理\n",
    "proxy = {\n",
    "    'http': 'http://127.0.0.1:7890',\n",
    "    'https': 'http://127.0.0.1:7890',\n",
    "}\n",
    "\n",
    "\n",
    "dir_path = \"./new_tweets/\"\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path) \n",
    "\n",
    "tweet_file = f\"{Keyword.replace(' ', ' - ')}_{Start_date}_{End_date}.json\".replace(\" \", \"\")\n",
    "user_info_file = f\"{Keyword.replace(' ', ' - ')}_{Start_date}_{End_date}_user_info.json\".replace(\" \", \"\")\n",
    "comments_file = f\"{Keyword.replace(' ', ' - ')}_{Start_date}_{End_date}_comments.json\".replace(\" \", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import sys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from rich import print\n",
    "\n",
    "class crawler:    \n",
    "       \n",
    "    def __init__(self, keyword, start_date, end_date, email, username, password, path, debug) -> None:\n",
    "        self.keyword = keyword\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.email = email\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.path = path\n",
    "        self.chrome_options = webdriver.ChromeOptions()\n",
    "        my_user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        service = Service(executable_path=ChromeDriverManager().install())\n",
    "        self.chrome_options.add_argument(\"--log-level=3\")\n",
    "        self.chrome_options.add_argument(f\"--user-agent={my_user_agent}\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        if not debug:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.driver = webdriver.Chrome(service=service, options=self.chrome_options)\n",
    "        self.tweets_data_list = []\n",
    "        self.error_count = 0\n",
    "\n",
    "\n",
    "    # def debug(self):\n",
    "    #     with open(\"test.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    #         file.write(self.driver.page_source)\n",
    "\n",
    "    def parse_count_string(self, count_str) -> int:\n",
    "        multipliers = 1\n",
    "        if \"K\" in count_str:\n",
    "            multipliers = 1000\n",
    "        elif \"M\" in count_str:\n",
    "            multipliers = 1000000\n",
    "        count_str = count_str.replace(\"K\", \"\").replace(\"M\", \"\")\n",
    "\n",
    "        try:\n",
    "            count = int(float(count_str) * multipliers)\n",
    "            return count\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def get_articles(self,  timeout: int, target_count: int):\n",
    "        \n",
    "        flag = 0\n",
    "        id_set = []\n",
    "        timeout = timeout\n",
    "        last_write_time = time.time()\n",
    "        current_count = 0\n",
    "        while True:\n",
    "            # print(f\"Found {len(tweets_data_list)} tweet(s) currently.\", end=\"\\r\")\n",
    "            current_time = time.time()\n",
    "            articles = self.driver.find_elements(By.TAG_NAME, \"article\")\n",
    "\n",
    "            for article in articles:\n",
    "                data = {\n",
    "                    \"text\": \"\",\n",
    "                    \"likes\": 0,\n",
    "                    \"replies\": 0,\n",
    "                    \"retweets\": 0,\n",
    "                    \"reads\": 0,\n",
    "                    \"publish_time\": \"\",\n",
    "                    \"url\": \"\",\n",
    "                    \"author\": \"\",\n",
    "                }\n",
    "                try:\n",
    "                    publish_time = article.find_element(\n",
    "                        By.TAG_NAME, \"time\"\n",
    "                    ).get_attribute(\"datetime\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if publish_time:\n",
    "                    data[\"publish_time\"] = publish_time.split(\"T\")[0]\n",
    "                try:\n",
    "                    tweet = article.find_element(\n",
    "                        By.CSS_SELECTOR, '[data-testid=\"tweetText\"]'\n",
    "                    )\n",
    "                except:\n",
    "                    continue\n",
    "                spans = tweet.find_elements(By.TAG_NAME, \"span\")\n",
    "                id = tweet.get_attribute(\"id\")\n",
    "\n",
    "                for span in spans:\n",
    "                    if \"r-qvk6io\" not in span.get_attribute(\n",
    "                        \"class\"\n",
    "                    ) and \"r-lrvibr\" not in span.get_attribute(\"class\"):\n",
    "                        data[\"text\"] += span.text.replace(\"\\n\", \"\")\n",
    "\n",
    "                likes = article.find_element(\n",
    "                    By.CSS_SELECTOR, '[data-testid=\"like\"]'\n",
    "                ).text\n",
    "                replies = article.find_element(\n",
    "                    By.CSS_SELECTOR, '[data-testid=\"reply\"]'\n",
    "                ).text\n",
    "                retweets = article.find_element(\n",
    "                    By.CSS_SELECTOR, '[data-testid=\"retweet\"]'\n",
    "                ).text\n",
    "                author = article.find_element(\n",
    "                    By.CSS_SELECTOR, '[data-testid=\"User-Name\"]'\n",
    "                ).text\n",
    "                reads = article.find_element(\n",
    "                    By.CSS_SELECTOR, '[data-testid=\"app-text-transition-container\"]'\n",
    "                ).text\n",
    "\n",
    "\n",
    "                if likes:\n",
    "                    data[\"likes\"] = self.parse_count_string(likes)\n",
    "                else:\n",
    "                    data[\"likes\"] = 0\n",
    "                if replies:\n",
    "                    data[\"replies\"] = self.parse_count_string(replies)\n",
    "                else:\n",
    "                    data[\"replies\"] = 0\n",
    "                if retweets:\n",
    "                    data[\"retweets\"] = self.parse_count_string(retweets)\n",
    "                else:\n",
    "                    data[\"retweets\"] = 0\n",
    "                if author:\n",
    "                    # 获取@到下一个\\n的字符串，包括@\n",
    "                    data[\"author\"] = author.split(\"@\")[1].split(\"\\n\")[0]\n",
    "                else:\n",
    "                    data[\"author\"] = \"Unknown\"\n",
    "                if reads:\n",
    "                    data[\"reads\"] = self.parse_count_string(reads)\n",
    "            \n",
    "                try:\n",
    "                    # 查找包含 Tweet 路径的 <a> 标签\n",
    "                    link_element = article.find_element(By.CSS_SELECTOR, 'a[href*=\"/status/\"]')\n",
    "                    href = link_element.get_attribute(\"href\")  # 获取 href 属性值\n",
    "                    tweet_id = href.split(\"/\")[-1]  # 提取 Tweet ID\n",
    "                    data[\"url\"] = f\"https://x.com/{data['author']}/status/{tweet_id}\"  # 构建 Tweet URL\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting tweet URL: {e}\")\n",
    "                    data[\"url\"] = \"Unknown\"\n",
    "\n",
    "                if id not in id_set:\n",
    "                    id_set.append(id)\n",
    "                    self.tweets_data_list.append(data)\n",
    "                    last_write_time = time.time()\n",
    "                    current_count += 1\n",
    "            \n",
    "            if current_count >= target_count:\n",
    "                break\n",
    "            \n",
    "            if current_time - last_write_time > timeout:\n",
    "                flag = 1\n",
    "            if flag == 1:\n",
    "                break\n",
    "\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"article\"))\n",
    "            )\n",
    "\n",
    "            body = self.driver.find_element(By.TAG_NAME, \"body\")\n",
    "            if body:\n",
    "                try:\n",
    "                    # body.send_keys(Keys.PAGE_DOWN)\n",
    "                    self.driver.execute_script(\n",
    "                        \"window.scrollTo(0,  document.body.scrollHeight);\"\n",
    "                    )\n",
    "                    time.sleep(4)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # body.send_keys(Keys.PAGE_DOWN)\n",
    "                    self.driver.execute_script(\n",
    "                        \"window.scrollTo(0,  document.body.scrollHeight);\"\n",
    "                    )\n",
    "                    time.sleep(4)\n",
    "            time.sleep(0.5)  \n",
    "\n",
    "\n",
    "        \n",
    "    def write_to_json(self)->None:\n",
    "        dir = self.path\n",
    "        path = dir + f\"{self.keyword.replace(' ', ' - ')}_{self.start_date}_{self.end_date}.json\".replace(\n",
    "            \" \", \"\"\n",
    "        )\n",
    "\n",
    "        tweets_dict = {\n",
    "            f\"tweet_{i + 1}\": tweet_data \n",
    "            for i, tweet_data in enumerate(self.tweets_data_list)\n",
    "        }\n",
    "        print(f\"\\nWriting to json...\")\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(tweets_dict, file, ensure_ascii=False, indent=4)\n",
    "        print(\"Complete!!!\")\n",
    "     \n",
    "  \n",
    "    def crawl(self) -> None:\n",
    "\n",
    "        current_date = datetime.datetime.strptime(self.start_date, \"%Y-%m-%d\").date()\n",
    "        current_next_date = current_date + datetime.timedelta(days=1)\n",
    "        start_date = datetime.datetime.strptime(self.start_date,  \"%Y-%m-%d\").date()\n",
    "        end_date=datetime.datetime.strptime(self.end_date,  \"%Y-%m-%d\").date()\n",
    "        total_days = (end_date - start_date).days + 1\n",
    "         \n",
    "        progress_bar_length = 50\n",
    "               \n",
    "        while current_date <= end_date:\n",
    "                     \n",
    "            email = self.email\n",
    "            username = self.username\n",
    "            password = self.password\n",
    "            \n",
    "            current_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            current_next_date_str = current_next_date.strftime(\"%Y-%m-%d\")\n",
    "            keyword = self.keyword\n",
    "\n",
    "            since = \"since:\" + current_date_str\n",
    "            until = \"until:\" + current_next_date_str\n",
    "            search = f\"{keyword} {since} {until}\".replace(\" \", \"%20\").replace(\n",
    "                \":\", \"%3A\"\n",
    "            )\n",
    "            url = f\"https://twitter.com/search?q={search}\"\n",
    "                       \n",
    "            self.driver.get(url)\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \".r-30o5oe.r-1dz5y72.r-13qz1uu.r-1niwhzg.r-17gur6a.r-1yadl64.r-deolkf.r-homxoj.r-poiln3.r-7cikom.r-1ny4l3l.r-t60dpp.r-fdjqy7\",\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                print(f\"Trying to login...\", end=\"\\r\")\n",
    "                email_input = self.driver.find_element(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \".r-30o5oe.r-1dz5y72.r-13qz1uu.r-1niwhzg.r-17gur6a.r-1yadl64.r-deolkf.r-homxoj.r-poiln3.r-7cikom.r-1ny4l3l.r-t60dpp.r-fdjqy7\",\n",
    "                )\n",
    "                \n",
    "                email_input.send_keys(email)\n",
    "                email_input.send_keys(Keys.ENTER)\n",
    "\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \".r-30o5oe.r-1dz5y72.r-13qz1uu.r-1niwhzg.r-17gur6a.r-1yadl64.r-deolkf.r-homxoj.r-poiln3.r-7cikom.r-1ny4l3l.r-t60dpp.r-fdjqy7\",\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                check_user = self.driver.find_element(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \".r-30o5oe.r-1dz5y72.r-13qz1uu.r-1niwhzg.r-17gur6a.r-1yadl64.r-deolkf.r-homxoj.r-poiln3.r-7cikom.r-1ny4l3l.r-t60dpp.r-fdjqy7\",\n",
    "                )\n",
    "                check_user.send_keys(username)\n",
    "                check_user.send_keys(Keys.ENTER)\n",
    "\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \".r-30o5oe.r-1dz5y72.r-13qz1uu.r-1niwhzg.r-17gur6a.r-1yadl64.r-deolkf.r-homxoj.r-poiln3.r-7cikom.r-1ny4l3l.r-t60dpp.r-fdjqy7\",\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                password_input = self.driver.find_element(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \".r-30o5oe.r-1dz5y72.r-13qz1uu.r-1niwhzg.r-17gur6a.r-1yadl64.r-deolkf.r-homxoj.r-poiln3.r-7cikom.r-1ny4l3l.r-t60dpp.r-fdjqy7\",\n",
    "                )\n",
    "                password_input.send_keys(password)\n",
    "                password_input.send_keys(Keys.ENTER)\n",
    "                print(f\"Login successfully\", end=\"\\r\")\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                print(\"Please double check your login information\")\n",
    "                exit()\n",
    "        \n",
    "            days_passed = (current_date - start_date ).days\n",
    "            progress_percent = (days_passed / total_days) * 100 \n",
    "            progress_bar = \"#\" * int(progress_percent / 100 * progress_bar_length)           \n",
    "            \n",
    "           \n",
    "            try:\n",
    "                error_element = self.driver.find_element(By.XPATH, '//span[contains(text(), \"Something went wrong\")]')                   \n",
    "            except:\n",
    "                error_element = None\n",
    "                \n",
    "            if error_element is not None:\n",
    "                    self.error_count += 1\n",
    "                    os.system('cls')\n",
    "                    print(f\"Something went wrong. I'll try it again... -> {self.error_count} retries\")\n",
    "                    print(f\"Progress: [{progress_percent:.2f} %] [{progress_bar.ljust(progress_bar_length)}]\", end=\"\\r\")\n",
    "                    # return\n",
    "                    continue\n",
    "           \n",
    "            \n",
    "\n",
    "            WebDriverWait(self.driver, 50).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, '[data-testid=\"tweetText\"]')\n",
    "                )\n",
    "            )\n",
    "            self.get_articles(10, 15)  \n",
    "                   \n",
    "         \n",
    "            current_date = current_next_date\n",
    "            current_next_date += datetime.timedelta(days=1)\n",
    "            self.error_count = 0\n",
    "\n",
    "            # 打印进度条\n",
    "            days_passed = (current_date - start_date ).days\n",
    "            progress_percent = (days_passed / total_days) * 100 \n",
    "            progress_bar = \"#\" * int(progress_percent / 100 * progress_bar_length)           \n",
    "            print(f\"Progress: [{progress_percent:.2f} %] [{progress_bar.ljust(progress_bar_length)}]\", end=\"\\r\")\n",
    "\n",
    "x_crawler = crawler(\n",
    "    keyword=Keyword,\n",
    "    start_date=Start_date,\n",
    "    end_date=End_date,\n",
    "    email = Email,\n",
    "    username = Username,\n",
    "    password = Password,\n",
    "    path=dir_path,\n",
    "    debug=False # This means a headless browser will be used.\n",
    ")\n",
    "x_crawler.crawl()\n",
    "x_crawler.write_to_json()\n",
    "x_crawler.driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户信息检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_crawl_tools import UserInfoScraper\n",
    "\n",
    "# 初始化用户信息抓取器\n",
    "user_scraper = UserInfoScraper(proxy, cookie, dir_path)\n",
    "\n",
    "# 抓取用户信息\n",
    "user_scraper.scrape_user_info(\n",
    "    input_filename=tweet_file,\n",
    "    output_filename=user_info_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_crawl_tools import CommentScraper\n",
    "\n",
    "# 初始化评论抓取器\n",
    "comment_scraper = CommentScraper(proxy, cookie, dir_path)\n",
    "\n",
    "# 抓取评论\n",
    "comment_scraper.scrape_comments(\n",
    "    input_filename=tweet_file,\n",
    "    output_filename=comments_file\n",
    ")\n",
    "\n",
    "print(\"所有数据抓取完成。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
