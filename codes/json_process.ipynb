{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理（json2xlsx）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main data saved to ./new_tweets/black-myth_2024-08-20_2024-09-30.json\n",
      "Comments saved to ./new_tweets/black-myth_2024-08-20_2024-09-30_comments.json\n",
      "User info saved to ./new_tweets/black-myth_2024-08-20_2024-09-30_user_info.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def merge_json_files(keyword, dir_path, output_path_prefix):\n",
    "    \"\"\"\n",
    "    合并目录下某关键词相关的 JSON 文件，分别合并主 JSON、评论和用户信息。\n",
    "    \n",
    "    :param keyword: 关键词，例如 'liziqi' 或 'black-myth'\n",
    "    :param dir_path: 输入目录路径\n",
    "    :param output_path_prefix: 输出文件路径前缀，例如 './new_tweets/liziqi_merged'\n",
    "    \"\"\"\n",
    "    # 筛选出与关键词相关的文件\n",
    "    keyword_files = [f for f in os.listdir(dir_path) if f.startswith(keyword)]\n",
    "    main_files = sorted([f for f in keyword_files if \"_comments\" not in f and \"_user_info\" not in f])\n",
    "    comment_files = sorted([f for f in keyword_files if \"_comments\" in f])\n",
    "    user_info_files = sorted([f for f in keyword_files if \"_user_info\" in f])\n",
    "\n",
    "    # 合并 JSON 数据\n",
    "    merged_main_data = []  # 用列表存储主数据\n",
    "    merged_comment_data = {}\n",
    "    merged_user_info_data = []\n",
    "\n",
    "    # 合并主文件\n",
    "    for file in main_files:\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            # 将所有 Tweet 数据合并到列表中\n",
    "            merged_main_data.extend(data.values())\n",
    "\n",
    "    # 合并评论文件\n",
    "    for file in comment_files:\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_comment_data.update(data)\n",
    "\n",
    "    # 合并用户信息文件\n",
    "    for file in user_info_files:\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_user_info_data.extend(data)\n",
    "\n",
    "    # 推断起始和终止日期\n",
    "    def extract_date_range(files):\n",
    "        dates = []\n",
    "        for filename in files:\n",
    "            parts = filename.split(\"_\")\n",
    "            if len(parts) >= 3:\n",
    "                try:\n",
    "                    dates.append(parts[1])\n",
    "                    dates.append(parts[2].split(\".\")[0])\n",
    "                except IndexError:\n",
    "                    pass\n",
    "        dates = sorted(set(datetime.strptime(d, \"%Y-%m-%d\") for d in dates))\n",
    "        return dates[0].strftime(\"%Y-%m-%d\"), dates[-1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    start_date, end_date = extract_date_range(main_files)\n",
    "\n",
    "    # 输出文件路径\n",
    "    main_output_path = f\"{output_path_prefix}_{start_date}_{end_date}.json\"\n",
    "    comments_output_path = f\"{output_path_prefix}_{start_date}_{end_date}_comments.json\"\n",
    "    user_info_output_path = f\"{output_path_prefix}_{start_date}_{end_date}_user_info.json\"\n",
    "\n",
    "    # 写入文件\n",
    "    with open(main_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_main_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Main data saved to {main_output_path}\")\n",
    "\n",
    "    with open(comments_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_comment_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Comments saved to {comments_output_path}\")\n",
    "\n",
    "    with open(user_info_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_user_info_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"User info saved to {user_info_output_path}\")\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "dir_path = \"./new_tweets\"\n",
    "# merge_json_files(\"liziqi\", dir_path, \"./new_tweets/liziqi\")\n",
    "merge_json_files(\"black-myth\", dir_path, \"./new_tweets/black-myth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main DataFrame Columns: Index(['text', 'likes', 'replies', 'retweets', 'reads', 'publish_time', 'url',\n",
      "       'author'],\n",
      "      dtype='object')\n",
      "Comments DataFrame Columns: Index(['_id', 'pub_time', 'source_url', 'pictures', 'video_url',\n",
      "       'play_duration', 'up_count', 'cmt_count', 'read_count', 'rtt_count',\n",
      "       'share_count', 'collect_count', 'quote_count', 'play_count',\n",
      "       'is_retweet', 'is_quote', 'embed_url', 'language', 'content',\n",
      "       'ref_article_id', 'ref_source_url', 'root_article_id',\n",
      "       'root_source_url', 'author_info.user_id', 'author_info.user_name',\n",
      "       'author_info.user_url', 'retweet_info._id',\n",
      "       'retweet_info.retweet_author.user_id',\n",
      "       'retweet_info.retweet_author.user_name', 'retweet_info.url',\n",
      "       'quote_info._id', 'quote_info.quote_author.user_id',\n",
      "       'quote_info.quote_author.user_name', 'quote_info.url',\n",
      "       'ref_author_info.user_id', 'ref_author_info.user_name',\n",
      "       'ref_author_info.user_url', 'root_author_info.user_id',\n",
      "       'root_author_info.user_name', 'root_author_info.user_url',\n",
      "       'source_tweet_id', 'source_tweet_text'],\n",
      "      dtype='object')\n",
      "User DataFrame Columns: Index(['user_id', 'user_name', 'display_name', 'user_url', 'profile_picture',\n",
      "       'header_photo', 'bio', 'location', 'website_link', 'birthday',\n",
      "       'followers_count', 'following_count', 'tweets_count', 'likes_count',\n",
      "       'media_count', 'listed_count', 'join_date', 'auth', 'professional',\n",
      "       'protected'],\n",
      "      dtype='object')\n",
      "Excel file saved to ./new_tweets/black-myth_2024-08-20_2024-09-30.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_excel_from_json(main_json_path, comments_json_path, user_json_path, output_excel_path):\n",
    "    \"\"\"\n",
    "    根据主 JSON、评论 JSON 和用户 JSON 生成 Excel 表格。\n",
    "    \"\"\"\n",
    "    # 加载数据\n",
    "    main_df = json_to_dataframe(main_json_path)\n",
    "    comments_df = json_to_dataframe(comments_json_path, additional_columns={\"source_tweet_id\": None, \"source_tweet_text\": None})\n",
    "    user_df = json_to_dataframe(user_json_path)\n",
    "\n",
    "    # 调试列名\n",
    "    print(\"Main DataFrame Columns:\", main_df.columns)\n",
    "    print(\"Comments DataFrame Columns:\", comments_df.columns)\n",
    "    print(\"User DataFrame Columns:\", user_df.columns)\n",
    "\n",
    "    # 修正 comments_df 的用户主键\n",
    "    if \"author_info.user_id\" in comments_df.columns:\n",
    "        comments_df.rename(columns={\"author_info.user_id\": \"author\"}, inplace=True)\n",
    "\n",
    "    # 添加源推特信息到评论数据\n",
    "    for idx, row in comments_df.iterrows():\n",
    "        source_tweet_id = row.get(\"source_tweet_id\", None)\n",
    "        if not source_tweet_id:\n",
    "            continue\n",
    "        source_tweet = main_df[main_df[\"url\"].str.contains(source_tweet_id, na=False)]\n",
    "        if not source_tweet.empty:\n",
    "            comments_df.at[idx, \"source_tweet_text\"] = source_tweet.iloc[0][\"text\"]\n",
    "\n",
    "    # 合并用户信息到主数据\n",
    "    main_df = merge_user_info(main_df, user_df, user_key=\"author\")\n",
    "\n",
    "    # 合并用户信息到评论数据\n",
    "    comments_df = merge_user_info(comments_df, user_df, user_key=\"author\")\n",
    "\n",
    "    # 重排序评论数据列\n",
    "    comment_columns = [\"content\", \"author\"] + [col for col in comments_df.columns if col not in [\"content\", \"author\"]]\n",
    "    comments_df = comments_df[comment_columns]\n",
    "\n",
    "    # 输出到 Excel\n",
    "    with pd.ExcelWriter(output_excel_path) as writer:\n",
    "        main_df.to_excel(writer, sheet_name=\"Main Data\", index=False)\n",
    "        comments_df.to_excel(writer, sheet_name=\"Comments\", index=False)\n",
    "        user_df.to_excel(writer, sheet_name=\"Users\", index=False)\n",
    "    print(f\"Excel file saved to {output_excel_path}\")\n",
    "\n",
    "\n",
    "def json_to_dataframe(json_path, additional_columns=None):\n",
    "    \"\"\"\n",
    "    加载 JSON 文件并转换为 DataFrame。\n",
    "    :param json_path: JSON 文件路径\n",
    "    :param additional_columns: 需要添加的额外列，字典形式\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        # 展平评论数据\n",
    "        data_lists = [value if len(value) > 0 else None for value in data.values()]\n",
    "        data_lists = [value for value in data_lists if value is not None]\n",
    "        all_comments = [comment for sublist in data_lists for comment in sublist]\n",
    "        df = pd.json_normalize(all_comments)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid JSON structure.\")\n",
    "    \n",
    "    if additional_columns:\n",
    "        for col, default_value in additional_columns.items():\n",
    "            df[col] = default_value\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_user_info(df, user_df, user_key=\"author\"):\n",
    "    \"\"\"\n",
    "    将用户信息合并到主数据或评论数据中。\n",
    "    :param df: 数据 DataFrame\n",
    "    :param user_df: 用户信息 DataFrame\n",
    "    :param user_key: 用户 ID 的列名\n",
    "    :return: 合并后的 DataFrame\n",
    "    \"\"\"\n",
    "    if user_key not in df.columns:\n",
    "        raise KeyError(f\"Key '{user_key}' not found in DataFrame columns: {df.columns.tolist()}\")\n",
    "    if \"user_id\" not in user_df.columns:\n",
    "        raise KeyError(f\"Key 'user_id' not found in User DataFrame columns: {user_df.columns.tolist()}\")\n",
    "\n",
    "    return df.merge(\n",
    "        user_df,\n",
    "        how=\"left\",\n",
    "        left_on=user_key,\n",
    "        right_on=\"user_id\",  # 假设用户信息的唯一标识列名为 user_id\n",
    "        suffixes=(\"\", \"_user\"),\n",
    "    )\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "# main_json_path = \"./new_tweets/liziqi_2024-11-01_2024-11-16.json\"\n",
    "# comments_json_path = \"./new_tweets/liziqi_2024-11-01_2024-11-16_comments.json\"\n",
    "# user_json_path = \"./new_tweets/liziqi_2024-11-01_2024-11-16_user_info.json\"\n",
    "# output_excel_path = \"./new_tweets/liziqi_2024-11-01_2024-11-16.xlsx\"\n",
    "\n",
    "# black-myth\n",
    "main_json_path = \"./new_tweets/black-myth_2024-08-20_2024-09-30.json\"\n",
    "comments_json_path = \"./new_tweets/black-myth_2024-08-20_2024-09-30_comments.json\"\n",
    "user_json_path = \"./new_tweets/black-myth_2024-08-20_2024-09-30_user_info.json\"\n",
    "output_excel_path = \"./new_tweets/black-myth_2024-08-20_2024-09-30.xlsx\"\n",
    "\n",
    "generate_excel_from_json(main_json_path, comments_json_path, user_json_path, output_excel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
